{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txg4dtCfez9F"
      },
      "outputs": [],
      "source": [
        "# week4_batch_predict.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "\n",
        "print(\"Daily Batch Churn Prediction\")\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"DailyChurn\").getOrCreate()\n",
        "\n",
        "# Load saved pipeline (Week 3)\n",
        "model = PipelineModel.load(\"/content/sparkscale_model\")\n",
        "\n",
        "# Simulated batch input (feature-ready)\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"avg_monthly_90d\", DoubleType()),\n",
        "    StructField(\"total_tenure\", IntegerType()),\n",
        "    StructField(\"max_bill\", DoubleType()),\n",
        "    StructField(\"session_count\", DoubleType()),\n",
        "    StructField(\"high_value\", DoubleType()),\n",
        "    StructField(\"Churn\", DoubleType())  # dummy column\n",
        "])\n",
        "\n",
        "new_users_data = [\n",
        "    (1, 45.0, 24, 150.5, 120.0, 1.0, 0.0),\n",
        "    (2, 30.0, 12, 85.2, 45.0, 0.0, 0.0),\n",
        "    (3, 65.0, 36, 220.1, 200.0, 1.0, 0.0),\n",
        "    (4, 92.5, 5, 115.0, 28.0, 1.0, 0.0)\n",
        "]\n",
        "\n",
        "new_data = spark.createDataFrame(new_users_data, schema)\n",
        "\n",
        "# Batch inference\n",
        "preds = model.transform(new_data)\n",
        "\n",
        "# Sanity check â€“ probability MUST appear\n",
        "preds.select(\n",
        "    \"id\",\n",
        "    \"probability\",\n",
        "    \"prediction\"\n",
        ").show(truncate=False)\n",
        "\n",
        "# Final batch output (CSV)\n",
        "preds.select(\n",
        "    col(\"id\"),\n",
        "    (vector_to_array(col(\"probability\"))[1] * 100).alias(\"churn_risk_percent\"),\n",
        "    col(\"prediction\")\n",
        ").coalesce(1).write.mode(\"overwrite\") \\\n",
        " .option(\"header\", \"true\") \\\n",
        " .csv(\"daily_predictions_out\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  DASHBOARD Representation.\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Dashboard\").getOrCreate()\n",
        "\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "df_viz = df_ml_ready.sample(0.1).toPandas()\n",
        "preds_viz = lr_pred.select(\"Churn\", \"prediction\", \"probability\").toPandas()\n",
        "\n",
        "# 2x3 Layout (Slightly Larger)\n",
        "fig = make_subplots(rows=2, cols=3,\n",
        "                   subplot_titles=[\"Churn\", \"AUC\", \"ROC\", \"Correlation\", \"Confusion\", \"Risk Bands\"],\n",
        "                   vertical_spacing=0.12, horizontal_spacing=0.09)\n",
        "\n",
        "# TOP ROW: Churn | AUC | ROC\n",
        "churn_counts = df_viz[\"Churn\"].value_counts()\n",
        "fig.add_trace(go.Bar(x=churn_counts.index, y=churn_counts.values,\n",
        "                    marker_color=[\"#10B981\",\"#EF4444\"],\n",
        "                    text=churn_counts.values, textposition=\"auto\"), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Bar(x=[\"LR\",\"RF\"], y=[lr_auc, rf_auc],\n",
        "                    marker_color=[\"#3B82F6\",\"#10B981\"],\n",
        "                    text=[f\"{lr_auc:.3f}\",f\"{rf_auc:.3f}\"], textposition=\"auto\"), row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=[0,lr_auc,1],y=[0,lr_auc,1], line=dict(color=\"#3B82F6\",width=3), showlegend=False), row=1, col=3)\n",
        "fig.add_trace(go.Scatter(x=[0,rf_auc,1],y=[0,rf_auc,1], line=dict(color=\"#10B981\",width=3), showlegend=False), row=1, col=3)\n",
        "fig.add_trace(go.Scatter(x=[0,1],y=[0,1], line=dict(color=\"lightgrey\",dash=\"dash\"), showlegend=False), row=1, col=3)\n",
        "\n",
        "# BOTTOM ROW: Corr | Confusion | Risk\n",
        "corr_df = df_viz.select_dtypes(np.number).corr()\n",
        "top4 = corr_df.columns[:4]\n",
        "corr4 = corr_df.loc[top4,top4].round(2)\n",
        "fig.add_trace(go.Heatmap(z=corr4.values,x=top4,y=top4,colorscale=\"Viridis\",\n",
        "                        text=corr4.values,texttemplate=\"%{text}\",textfont_size=11,\n",
        "                        showscale=False), row=2, col=1)\n",
        "\n",
        "cm_df = pd.crosstab(preds_viz[\"prediction\"], preds_viz[\"Churn\"])\n",
        "fig.add_trace(go.Heatmap(z=cm_df.values,x=cm_df.columns,y=cm_df.index,\n",
        "                        colorscale=\"Blues\",text=cm_df.values,texttemplate=\"%{text}\",\n",
        "                        textfont_size=13,showscale=False), row=2, col=2)\n",
        "\n",
        "# Risk Distribution (NEW!)\n",
        "risk_scores = (preds_viz[\"probability\"].apply(lambda x: x[1]*100)).round(0)\n",
        "risk_bands = pd.cut(risk_scores, bins=5, labels=[\"0-20%\", \"21-40%\", \"41-60%\", \"61-80%\", \"81-100%\"])\n",
        "risk_counts = risk_bands.value_counts().sort_index()\n",
        "fig.add_trace(go.Bar(x=risk_counts.index, y=risk_counts.values,\n",
        "                    marker_color=[\"#10B981\",\"#F59E0B\",\"#F97316\",\"#EF4444\",\"#DC2626\"],\n",
        "                    text=risk_counts.values, textposition=\"auto\"), row=2, col=3)\n",
        "\n",
        "# SLIGHTLY LARGER (PPT Optimized)\n",
        "fig.update_layout(height=850, width=1200,  # ðŸ‘ˆ Perfect increase!\n",
        "                 title=\"SPARKSCALE â€“  Production Dashboard\",\n",
        "                 title_x=0.5, title_font_size=24, font_size=13,\n",
        "                 showlegend=False, margin=dict(l=30,r=30,t=70,b=30))\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "XRSOn5StfSS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Churn PREDICT\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"avg_monthly_90d\", DoubleType()),\n",
        "    StructField(\"total_tenure\", DoubleType()),\n",
        "    StructField(\"max_bill\", DoubleType()),\n",
        "    StructField(\"session_count\", DoubleType()),\n",
        "    StructField(\"high_value\", DoubleType()),\n",
        "    StructField(\"Churn\", DoubleType())\n",
        "])\n",
        "\n",
        "# YOUR CUSTOMER (edit)\n",
        "customer_row = [92.5, 5.0, 115.0, 28.0, 1.0, 0.0]\n",
        "\n",
        "single_df = spark.createDataFrame([customer_row], schema)\n",
        "result_df = rf_model.transform(single_df)\n",
        "\n",
        "# CLEAN OUTPUT\n",
        "risk_prob = result_df.select(\"probability\").collect()[0][0][1]\n",
        "pred = result_df.select(\"prediction\").collect()[0][0]\n",
        "\n",
        "print(\"SPARKSCALE RESULT\")\n",
        "print(f\"90d Avg: â‚¹{customer_row[0]} | Tenure: {customer_row[1]}m | High Value: {customer_row[4]}\")\n",
        "print(f\"CHURN RISK: {risk_prob:.1%}\")\n",
        "print(f\"Prediction: {'CHURN RETAIN!' if pred==1.0 else 'SAFE '}\")\n",
        "\n",
        "result_df.select(\"probability\", \"prediction\").show(truncate=False)\n"
      ],
      "metadata": {
        "id": "pZcCQt7BfVBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDbOY7VLfXSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}