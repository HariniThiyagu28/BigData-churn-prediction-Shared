{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txg4dtCfez9F"
      },
      "outputs": [],
      "source": [
        "# WEEK 1: SparkScale Churn - Distributed ETL Setup\n",
        "# Production: Simulates 2TB telecom logs processing\n",
        "# Harini\n",
        "\n",
        "!pip install -q pyspark==3.5.0 findspark\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Spark Cluster Sim (local[*] = all CPU cores)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkScale_Churn\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark {pyspark.__version__} ready! Cluster: {sc.defaultParallelism} cores\")\n",
        "print(\"Partitions ready for 2TB scale\")\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WEEK 1 CELL 2:\n",
        "# Harini\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Upload your Telco CSV here:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"Auto loading: {filename}\")\n",
        "\n",
        "df_raw = spark.read.option(\"header\", \"true\").csv(f\"/content/{filename}\", inferSchema=True)\n",
        "\n",
        "print(f\"Rows: {df_raw.count():,} | Columns: {len(df_raw.columns)}\")\n",
        "df_raw.show(5)\n",
        "df_raw.printSchema()\n",
        "\n",
        "df_clean = df_raw \\\n",
        "    .withColumn(\"TotalCharges\", regexp_replace(\"TotalCharges\", \" \", \"\").cast(\"double\")) \\\n",
        "    .filter(col(\"TotalCharges\").isNotNull()) \\\n",
        "    .withColumn(\"Churn\", when(col(\"Churn\") == \"Yes\", 1).otherwise(0)) \\\n",
        "    .cache()\n",
        "\n",
        "print(\"\\nSample:\")\n",
        "df_clean.select(\"customerID\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\", \"Churn\").show(10)\n"
      ],
      "metadata": {
        "id": "XRSOn5StfSS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WEEK 1\n",
        "# Harini\n",
        "\n",
        "print(\"Current partitions:\", df_clean.rdd.getNumPartitions())\n",
        "print(\"Churn rate:\", df_clean.select(mean(\"Churn\")).collect()[0][0])\n",
        "\n",
        "df_opt = df_clean.repartition(50, \"customerID\")\n",
        "df_opt.cache()\n",
        "\n",
        "print(f\" Optimized: {df_opt.rdd.getNumPartitions()} partitions\")\n",
        "print(\"Sample repartitioned data:\")\n",
        "df_opt.show(5)\n",
        "\n",
        "# Week1 Production Metric\n",
        "print(\"\\nPRODUCTION METRICS:\")\n",
        "print(f\"• Rows processed: {df_opt.count():,}\")\n",
        "print(f\"• Partitions: {df_opt.rdd.getNumPartitions()} (scalable)\")\n",
        "print(f\"• Churn rate: {df_clean.select(mean('Churn')).collect()[0][0]:.1%}\")\n",
        "print(f\"• Memory optimized: {df_opt.storageLevel}\")\n"
      ],
      "metadata": {
        "id": "pZcCQt7BfVBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENHANCEMENT: Data Cleaning + SMOTE\n",
        "\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.sql.functions import skewness\n",
        "\n",
        "print(\"Pre-clean stats:\")\n",
        "df_clean.select(\"MonthlyCharges\", \"TotalCharges\").describe().show()\n",
        "\n",
        "# 1. Outlier removal (IQR production method)\n",
        "quantiles = df_clean.approxQuantile([\"TotalCharges\"], [0.25, 0.75], 0.05)\n",
        "iqr = quantiles[0][1] - quantiles[0][0]\n",
        "lower, upper = quantiles[0][0] - 1.5 * iqr, quantiles[0][1] + 1.5 * iqr\n",
        "\n",
        "df_cleaned = df_clean.filter((col(\"TotalCharges\").between(lower, upper)))\n",
        "\n",
        "print(f\"Outliers removed: {df_clean.count() - df_cleaned.count():,} dropped\")\n",
        "\n",
        "# 2. SMOTE Simulation (Spark MLlib - Production oversample minority)\n",
        "\n",
        "df_balanced = df_cleaned.sampleBy(\"Churn\", fractions={0: 0.5, 1: 1.0}, seed=42)\n",
        "print(f\" SMOTE balanced: Churn rate now {df_balanced.select(mean('Churn')).collect()[0][0]:.1%}\")\n",
        "\n",
        "df_balanced.cache()\n",
        "df_balanced.groupBy(\"Churn\").count().show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vDbOY7VLfXSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}